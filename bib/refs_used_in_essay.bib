

@book{THeyEtAl2009,
  author =        {Hey, Tony and Tansley, Stewart and Tolle, Kristin},
  month =         oct,
  title =         {The {{Fourth Paradigm}}: {{Data}}-{{Intensive
                   Scientific Discovery}}},
  year =          {2009},
  abstract =      {Increasingly, scientific breakthroughs will be
                   powered by advanced computing capabilities that help
                   researchers manipulate and explore massive datasets.
                   The speed at which any given scientific discipline
                   advances will depend on how well its researchers
                   collaborate with one another, and with technologists,
                   in areas of eScience such as databases, workflow
                   management, visualization, and cloud computing
                   technologies. [\ldots{}]},
  isbn =          {978-0-9825442-0-4},
  language =      {en-US},
}

@misc{JMcCarthyEtAl1955,
  author =        {McCarthy, J and Minsky, M L and Rochester, N and
                   Shannon, C E},
  title =         {A {{PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH
                   PROJECT ON ARTIFICIAL INTELLIGENCE}}},
  year =          {1955},
  language =      {en},
}

@article{WKangEtAl2019,
  author =        {Kang, Wei and Oshan, Taylor and Wolf, Levi J and
                   Boeing, Geoff and {Frias-Martinez}, Vanessa and
                   Gao, Song and Poorthuis, Ate and Xu, Wenfei},
  journal =       {Environment and Planning B: Urban Analytics and City
                   Science},
  month =         nov,
  number =        {9},
  pages =         {1756--1768},
  publisher =     {{SAGE Publications Ltd STM}},
  title =         {A Roundtable Discussion: {{Defining}} Urban Data
                   Science},
  volume =        {46},
  year =          {2019},
  abstract =      {The field of urban analytics and city science has
                   seen significant growth and development in the past
                   20 years. The rise of data science, both in industry
                   and academia, has put new pressures on urban
                   research, but has also allowed for new analytical
                   possibilities. Because of the rapid growth and change
                   in the field, terminology in urban analytics can be
                   vague and unclear. This paper, an abridged synthesis
                   of a panel discussion among scholars in Urban Data
                   Science held at the 2019 American Association of
                   Geographers Conference in Washington, D.C., outlines
                   one discussion seeking a better sense of the
                   conceptual, terminological, social, and ethical
                   challenges faced by researchers in this emergent
                   field. The panel outlines the difficulties of
                   defining what is or is not urban data science,
                   finding that good urban data science must have an
                   expansive role in a successful discipline of ``city
                   science.'' It suggests that ``data science'' has
                   value as a ``signaling'' term in industrial or
                   popular science applications, but which may not
                   necessarily be well-understood within purely academic
                   circles. The panel also discusses the normative value
                   of doing urban data science, linking successful
                   practice back to urban life. Overall, this panel
                   report contributes to the wider discussion around
                   urban analytics and city science and about the role
                   of data science in this domain.},
  doi =           {10.1177/2399808319882826},
  issn =          {2399-8083},
  language =      {en},
}

@article{ESheppard2014,
  author =        {Sheppard, Eric},
  journal =       {Urban Geography},
  month =         jul,
  note =          {\_eprint:
                   https://doi.org/10.1080/02723638.2014.916907},
  number =        {5},
  pages =         {636--644},
  publisher =     {{Routledge}},
  title =         {We Have Never Been Positivist\textdagger{}},
  volume =        {35},
  year =          {2014},
  abstract =      {The specter of positivism haunts critical urban
                   studies, distracting us from the possibilities of
                   creative, rigorous, critically engaged activist
                   scholarship beyond the obsolete dichotomies of
                   quantitative/qualitative methodologies and
                   positivist/post-positivist epistemologies. Yet a
                   genealogy of positivism shows that the movement was
                   never as philosophically coherent, or as politically
                   conservative, or as well-defined a research program
                   as portrayed in our intellectual histories: we have
                   never been positivist. The `post' in post-positivist
                   urban studies does not mark positivism's antithesis,
                   but instead refers to the variegated critical
                   ontologies and epistemologies always discomfited by a
                   positivism that never was. Creating space to move
                   beyond the inherited myths of positivist and
                   quantitative urban scholarship requires abandoning
                   either/or differentiations in favor of the both/and
                   possibilities of engaged pluralism.},
  doi =           {10.1080/02723638.2014.916907},
  issn =          {0272-3638},
}

@book{PLongleyEtAl1998,
  author =        {Longley, P. A. and Brooks, Susan and Macmillan, W. and
                   McDonnell, R. A.},
  editor =        {Longley, P. A. and Brooks, Susan and Macmillan, W. and
                   McDonnell, R. A.},
  publisher =     {{Wiley}},
  title =         {Geocomputation: A Primer},
  year =          {1998},
  abstract =      {Book synopsis: Geocomputation A Primer edited by Paul
                   A Longley Sue M Brooks Rachael McDonnell School of
                   Geographical Sciences, University of Bristol, UK and
                   Bill Macmillan School of Geography, University of
                   Oxford, UK This book encompasses all that is new in
                   geocomputation. It is also a primer - that is, a book
                   which sets out the foundations and scope of this
                   important emergent area from the same contemporary
                   perspective. The catalyst to the emergence of
                   geocomputation is the new and creative application of
                   computers to devise and depict digital
                   representations of the Earth's surface. The
                   environment for geocomputation is provided by
                   geographical information systems (GIS), yet
                   geocomputation is much more than GIS. Geocomputation
                   is a blend of research-led applications which
                   emphasise process over form, dynamics over statics,
                   and interaction over passive response. This book
                   presents a timely blend of current research and
                   practice, written by the leading figures in the
                   field. It provides insights to a new and rapidly
                   developing area, and identifies the key foundations
                   to future developments. It should be read by all who
                   seek to use geocomputational methods for solving real
                   world problems.},
  isbn =          {978-0-471-98575-4},
  language =      {en-GB},
}

@incollection{RNelson1962,
  address =       {{Princeton}},
  author =        {Nelson, Richard},
  booktitle =     {The {{Rate}} and {{Direction}} of {{Inventive
                   Activity}}: {{Economic}} and {{Social Factors}}},
  publisher =     {{Princeton University Press}},
  title =         {The {{Link Between Science}} and {{Invention}}: {{The
                   Case}} of the {{Transistor}}},
  year =          {1962},
  doi =           {10.1515/9781400879762},
  isbn =          {978-1-4008-7976-2},
  language =      {en},
}

@book{JKassing2011,
  author =        {Kassing, Jeffrey},
  month =         jul,
  publisher =     {{Polity}},
  title =         {Dissent in {{Organizations}}},
  year =          {2011},
  abstract =      {Employees often disagree with workplace policies and
                   practices, leaving few workplaces unaffected by
                   organizational dissent. While disagreement persists
                   in most contemporary organizations, how employees
                   express dissent at work and how their respective
                   organizations respond to it vary widely. Through the
                   use of case studies, first-person accounts, current
                   examples, conceptual models, and scholarly findings
                   this work offers a comprehensive treatment of
                   organizational dissent. Readers will find a sensible
                   balance between theoretical considerations and
                   practical applications. Theoretical considerations
                   include: how dissent fits within classical and
                   contemporary organizational communication approaches
                   dissent's relationship to, yet distinctiveness from,
                   related organizational concepts like conflict,
                   resistance, and voice explanations for why employees
                   express dissent and how they make sense of it the
                   relationship between organizational dissent and
                   ethics Practical applications encompass:
                   recommendations for employees expressing dissent and
                   managers responding to it consideration of the range
                   of events that trigger dissent strategies employees
                   use to express dissent and tools organizations can
                   apply to solicit it effectively the unique challenges
                   and benefits associated with expressing dissent to
                   management The book's specific focus and engaged
                   voice provide students, scholars, and practitioners
                   with a deeper understanding of dissent as an
                   important aspect of workplace communication.},
  isbn =          {978-0-7456-5139-2},
  language =      {en},
}

@article{WShockley1949,
  author =        {Shockley, W.},
  journal =       {Bell System Technical Journal},
  note =          {\_eprint:
  https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1949.tb03645.x},
  number =        {3},
  pages =         {435--489},
  title =         {The {{Theory}} of P-n {{Junctions}} in
                   {{Semiconductors}} and p-n {{Junction Transistors}}},
  volume =        {28},
  year =          {1949},
  abstract =      {In a single crystal of semiconductor the impurity
                   concentration may vary from p-type to n-type
                   producing a mechanically continuous rectifying
                   junction. The theory of potential distribution and
                   rectification for p-n junctions is developed with
                   emphasis on germanium. The currents across the
                   junction are carried by the diffusion of holes in
                   n-type material and electrons in p-type material,
                   resulting in an admittance for a simple case varying
                   as (1 + i{$\omega\tau$}p)1/2 where {$\tau$}p is the
                   lifetime of a hole in the n-region. Contact
                   potentials across p-n junctions, carrying no current,
                   may develop when hole or electron injection occurs.
                   The principles and theory of a p-n-p transistor are
                   described.},
  doi =           {10.1002/j.1538-7305.1949.tb03645.x},
  issn =          {1538-7305},
  language =      {en},
}

@article{ASamuel1959,
  author =        {Samuel, A. L.},
  journal =       {IBM Journal of Research and Development},
  month =         jul,
  number =        {3},
  pages =         {210--229},
  title =         {Some {{Studies}} in {{Machine Learning Using}} the
                   {{Game}} of {{Checkers}}},
  volume =        {3},
  year =          {1959},
  abstract =      {Two machine-learning procedures have been
                   investigated in some detail using the game of
                   checkers. Enough work has been done to verify the
                   fact that a computer can be programmed so that it
                   will learn to play a better game of checkers than can
                   be played by the person who wrote the program.
                   Furthermore, it can learn to do this in a remarkably
                   short period of time (8 or 10 hours of
                   machine-playing time) when given only the rules of
                   the game, a sense of direction, and a redundant and
                   incomplete list of parameters which are thought to
                   have something to do with the game, but whose correct
                   signs and relative weights are unknown and
                   unspecified. The principles of machine learning
                   verified by these experiments are, of course,
                   applicable to many other situations.},
  doi =           {10.1147/rd.33.0210},
  issn =          {0018-8646},
}

@article{ATuring1936,
  author =        {Turing, Alan},
  journal =       {Proceedings of the London Mathematical Society},
  number =        {42},
  title =         {On {{Computable Numbers}}, with an {{Application}} to
                   the {{Entscheidungsproblem}}},
  volume =        {2},
  year =          {1936},
}

@article{GMoore1965,
  author =        {Moore, G.E.},
  journal =       {Proceedings of the IEEE},
  number =        {1},
  pages =         {82--85},
  title =         {Cramming {{More Components Onto Integrated
                   Circuits}}},
  volume =        {86},
  year =          {1965},
  doi =           {10.1109/JPROC.1998.658762},
  issn =          {0018-9219, 1558-2256},
  language =      {en},
}

@book{DHilbertAckermann1928,
  author =        {Hilbert, David and Ackermann, Wilhelm},
  publisher =     {{J. Springer}},
  title =         {{Grundz{\"u}ge der theoretischen Logik}},
  year =          {1928},
  language =      {de},
}

@book{CBrunsdonSingleton2015,
  address =       {{1 Oliver's Yard,~55 City Road~London~EC1Y 1SP}},
  author =        {Brunsdon, Chris and Singleton, Alex},
  publisher =     {{SAGE Publications, Inc.}},
  title =         {Geocomputation: {{A Practical Primer}}},
  year =          {2015},
  doi =           {10.4135/9781473916432},
  isbn =          {978-1-4462-7293-0 978-1-4739-1643-2},
}

@article{AChurch1936,
  author =        {Church, Alonzo},
  journal =       {American Journal of Mathematics},
  number =        {2},
  pages =         {345--363},
  publisher =     {{Johns Hopkins University Press}},
  title =         {An {{Unsolvable Problem}} of {{Elementary Number
                   Theory}}},
  volume =        {58},
  year =          {1936},
  doi =           {10.2307/2371045},
  issn =          {0002-9327},
}

@article{FRosenblatt1958,
  author =        {Rosenblatt, F.},
  journal =       {Psychological Review},
  number =        {6},
  pages =         {386--408},
  title =         {The Perceptron: {{A}} Probabilistic Model for
                   Information Storage and Organization in the Brain.},
  volume =        {65},
  year =          {1958},
  doi =           {10.1037/h0042519},
  issn =          {1939-1471, 0033-295X},
  language =      {en},
}

@article{FAllen1981,
  author =        {Allen, F. E.},
  journal =       {IBM Journal of Research and Development},
  month =         sep,
  number =        {5},
  pages =         {535--548},
  title =         {The {{History}} of {{Language Processor Technology}}
                   in {{IBM}}},
  volume =        {25},
  year =          {1981},
  abstract =      {The history of language processor technology in IBM
                   is described in this paper. Most of the paper is
                   devoted to compiler technology; interpreters,
                   assemblers, and macro systems are discussed briefly.
                   The emphasis is on scientific contributions and
                   technological advances from a historical perspective.
                   The synergistic relationship between theory and
                   practice is a subtheme.},
  doi =           {10.1147/rd.255.0535},
  issn =          {0018-8646},
}

@incollection{GORegan2018,
  address =       {{Cham}},
  author =        {O'Regan, Gerard},
  booktitle =     {The {{Innovation}} in {{Computing Companion}}: {{A
                   Compendium}} of {{Select}}, {{Pivotal Inventions}}},
  editor =        {O'Regan, Gerard},
  pages =         {151--153},
  publisher =     {{Springer International Publishing}},
  title =         {Hollerith's {{Tabulating Machines}} and the {{Birth}}
                   of {{IBM}}},
  year =          {2018},
  abstract =      {Hollerith's punch card tabulating machine was
                   designed to process the results of the 1890 census in
                   the United States. It used an electric current to
                   sense holes in punched cards, and it kept a running
                   total of the data. The statistics could be recorded
                   by electrically reading and sorting the punched
                   cards, and the results of the census were available
                   in a couple of months rather than years. Hollerith
                   formed the Tabulating Machine Company (the first
                   electric tabulating-machine company) in 1896, and it
                   merged with the International Time Recording Company
                   to form the Computing-Tabulating-Recording Company
                   (CTR) in 1911. Thomas Watson joined the company in
                   1914, and the company changed its name to
                   International Business Machines (IBM) in 1924. IBM
                   has been in business for over 100 years and remains a
                   respected leader in the computing field.},
  doi =           {10.1007/978-3-030-02619-6_31},
  isbn =          {978-3-030-02619-6},
  language =      {en},
}

@incollection{DKnuthPardo1980,
  author =        {Knuth, Donald E. and Pardo, Luis Trabb},
  booktitle =     {A {{History}} of {{Computing}} in the {{Twentieth
                   Century}}},
  pages =         {197--273},
  publisher =     {{Elsevier}},
  title =         {The {{Early Development}} of {{Programming
                   Languages}}},
  year =          {1980},
  doi =           {10.1016/B978-0-12-491650-0.50019-8},
  isbn =          {978-0-12-491650-0},
  language =      {en},
}

@article{PHaggett2008,
  author =        {Haggett, Peter},
  journal =       {Geographical Analysis},
  note =          {\_eprint:
  https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1538-4632.2008.00731.x},
  number =        {3},
  pages =         {336--352},
  title =         {The {{Local Shape}} of {{Revolution}}:
                   {{Reflections}} on {{Quantitative Geography}} at
                   {{Cambridge}} in the 1950s and 1960s},
  volume =        {40},
  year =          {2008},
  abstract =      {The ``quantitative revolution'' in human geography
                   which swept across so many universities in the 1950s
                   and 1960s had its main diffusion centers in a few
                   locations which were to have global significance. Two
                   critical early centers were the University of
                   Washington in the Pacific Northwest and Lund
                   University in southern Sweden. But the experience of
                   change was different in different locations as the
                   general forces of perturbation sweeping around
                   academia were translated into local eddies with local
                   repercussions. Here, small and somewhat random quirks
                   at the outset, led eventually to fundamental
                   divergences between adoption and rejection. The theme
                   is illustrated by reference to changes which occurred
                   at Cambridge, one of England's two oldest
                   universities, as seen from the perspective of someone
                   who\textemdash{}as undergraduate, graduate student,
                   and later, faculty member\textemdash{}was caught up
                   in these changes and took some small part in
                   propagating them. Special attention is given to the
                   role of two environmental scientists, Vaughan Lewis
                   and Richard Chorley, in introducing changes and the
                   way in which later developments in human geography
                   drew on preceding experiences in physical geography.
                   The reasons behind the ``Cambridge variant'' and the
                   questions of how intellectual DNA is passed across
                   the generations are discussed.},
  doi =           {10.1111/j.1538-4632.2008.00731.x},
  issn =          {1538-4632},
  language =      {en},
}

@article{HMiller2004,
  author =        {Miller, Harvey J.},
  journal =       {Annals of the Association of American Geographers},
  month =         jun,
  note =          {\_eprint:
                   https://doi.org/10.1111/j.1467-8306.2004.09402005.x},
  number =        {2},
  pages =         {284--289},
  publisher =     {{Routledge}},
  title =         {Tobler's {{First Law}} and {{Spatial Analysis}}},
  volume =        {94},
  year =          {2004},
  doi =           {10.1111/j.1467-8306.2004.09402005.x},
  issn =          {0004-5608},
}

@article{DRitchieEtAl1978,
  author =        {Ritchie, D. M. and Johnson, S. C. and Lesk, M. E. and
                   Kernighan, B. W.},
  journal =       {The Bell System Technical Journal},
  month =         jul,
  number =        {6},
  pages =         {1991--2019},
  title =         {{{UNIX}} Time-Sharing System: {{The C}} Programming
                   Language},
  volume =        {57},
  year =          {1978},
  abstract =      {C is a general-purpose programming language that has
                   proven useful for a wide variety of applications. It
                   is the primary language of the UNIX{${_\ast}$}
                   system, and is also available in several other
                   environments. This paper provides an overview of the
                   syntax and semantics of C and a discussion of its
                   strengths and weaknesses.},
  doi =           {10.1002/j.1538-7305.1978.tb02140.x},
  issn =          {0005-8580},
}

@misc{KGodel1931,
  author =        {G{\"o}del, K.},
  publisher =     {{Monatshefte f{\"u}r mathematik und physik}},
  title =         {{\"U}ber Formal Unentscheidbare {{S{\"a}tze}} Der
                   {{Principia Mathematica}} Und Verwandter {{Systeme
                   I}}},
  year =          {1931},
}

@book{TSejnowski2018,
  author =        {Sejnowski, Terrence J.},
  month =         oct,
  publisher =     {{MIT Press}},
  title =         {The {{Deep Learning Revolution}}},
  year =          {2018},
  abstract =      {How deep learning\textemdash{}from Google Translate
                   to driverless cars to personal cognitive
                   assistants\textemdash{}is changing our lives and
                   transforming every sector of the economy.The deep
                   learning revolution has brought us driverless cars,
                   the greatly improved Google Translate, fluent
                   conversations with Siri and Alexa, and enormous
                   profits from automated trading on the New York Stock
                   Exchange. Deep learning networks can play poker
                   better than professional poker players and defeat a
                   world champion at Go. In this book, Terry Sejnowski
                   explains how deep learning went from being an arcane
                   academic field to a disruptive technology in the
                   information economy.Sejnowski played an important
                   role in the founding of deep learning, as one of a
                   small group of researchers in the 1980s who
                   challenged the prevailing logic-and-symbol based
                   version of AI. The new version of AI Sejnowski and
                   others developed, which became deep learning, is
                   fueled instead by data. Deep networks learn from data
                   in the same way that babies experience the world,
                   starting with fresh eyes and gradually acquiring the
                   skills needed to navigate novel environments.
                   Learning algorithms extract information from raw
                   data; information can be used to create knowledge;
                   knowledge underlies understanding; understanding
                   leads to wisdom. Someday a driverless car will know
                   the road better than you do and drive with more
                   skill; a deep learning network will diagnose your
                   illness; a personal cognitive assistant will augment
                   your puny human brain. It took nature many millions
                   of years to evolve human intelligence; AI is on a
                   trajectory measured in decades. Sejnowski prepares us
                   for a deep learning future.},
  isbn =          {978-0-262-03803-4},
  language =      {en},
}

@misc{RStallman1983,
  author =        {Stallman, R.},
  journal =       {net.unix-wizards},
  title =         {New {{UNIX}} Implementation},
  year =          {1983},
}

@article{RTomlinson1969,
  author =        {Tomlinson, R.},
  journal =       {Journal of Geography},
  number =        {1},
  pages =         {45--48},
  title =         {A {{Geographic Information System}} for {{Regional
                   Planning}}},
  volume =        {78},
  year =          {1969},
  abstract =      {Semantic Scholar extracted view of "A Geographic
                   Information System for Regional Planning" by R.
                   Tomlinson},
  doi =           {10.5026/jgeography.78.45},
}

@book{DHebb1949,
  address =       {{New York, NY, USA}},
  author =        {Hebb, D. O.},
  publisher =     {{Wiley}},
  title =         {The {{Organization}} of {{Behavior}}: {{A
                   Neuropsychological Theory}}},
  year =          {1949},
  language =      {en},
}

@article{CDierbach2014,
  author =        {Dierbach, Charles},
  journal =       {Journal of Computing Sciences in Colleges},
  month =         jan,
  number =        {3},
  pages =         {73},
  title =         {Python as a First Programming Language},
  volume =        {29},
  year =          {2014},
  abstract =      {The Python programming language has been around since
                   its development by Guido van Rossum around 1991. One
                   of the main features of the language is code
                   readability (sometimes described as "executable
                   pseudocode"). Python is an interactive programming
                   language, using dynamic typing. It is also a hybrid
                   language, supporting the imperative, object-oriented
                   and functional programming paradigms. Although used
                   as a scripting language, it is also used for the
                   development of full-scale programs.},
  issn =          {1937-4771},
}

@article{IBurton1963,
  author =        {Burton, Ian},
  journal =       {The Canadian Geographer/Le G{\'e}ographe canadien},
  month =         dec,
  number =        {4},
  pages =         {151--162},
  title =         {{{THE QUANTITATIVE REVOLUTION AND THEORETICAL
                   GEOGRAPHY}}},
  volume =        {7},
  year =          {1963},
  doi =           {10.1111/j.1541-0064.1963.tb00796.x},
  issn =          {0008-3658, 1541-0064},
  language =      {en},
}

@article{GBilesEtAl1989,
  author =        {Biles, George E. and Bolton, Alfred A. and
                   DiRe, Bernadette M.},
  journal =       {Journal of Management},
  month =         dec,
  number =        {4},
  pages =         {603--615},
  publisher =     {{SAGE Publications Inc}},
  title =         {Herman {{Hollerith}}: {{Inventor}}, {{Manager}},
                   {{Entrepreneur}}\textemdash{{A Centennial
                   Remembrance}}},
  volume =        {15},
  year =          {1989},
  abstract =      {Herman Hollerith developed electric tabulating
                   machines to be used in compiling, aggregating, and
                   totaling data itemsfor the 1890 United States census.
                   Hollerith's innovative genius and success with the
                   electric tabulation of complex data laid the
                   foundation for the computer industry and contributed
                   to the development of management information
                   systems.},
  doi =           {10.1177/014920638901500409},
  issn =          {0149-2063},
}

@incollection{PMancosuEtAl2009,
  author =        {Mancosu, Paolo and Zach, Richard and Badesa, Calixto},
  booktitle =     {The {{Development}} of {{Modern Logic}}},
  editor =        {Haaparanta, Leila},
  month =         jun,
  pages =         {318--470},
  publisher =     {{Oxford University Press}},
  title =         {The {{Development}} of {{Mathematical Logic}} from
                   {{Russell}} to {{Tarski}}, 1900\textendash{}1935},
  year =          {2009},
  doi =           {10.1093/acprof:oso/9780195137316.003.0029},
  isbn =          {978-0-19-513731-6},
  language =      {en},
}

@incollection{THeyTrefethen2003,
  address =       {{Chichester, UK}},
  author =        {Hey, Tony and Trefethen, Anne},
  booktitle =     {Wiley {{Series}} in {{Communications Networking}} \&
                   {{Distributed Systems}}},
  editor =        {Berman, Fran and Fox, Geoffrey and Hey, Tony},
  month =         mar,
  pages =         {809--824},
  publisher =     {{John Wiley \& Sons, Ltd}},
  title =         {The {{Data Deluge}}: {{An}} e-{{Science
                   Perspective}}},
  year =          {2003},
  abstract =      {This paper previews the imminent flood of scientific
                   data expected from the next generation of
                   experiments, simulations, sensors and satellites. In
                   order to be exploited by search engines and data
                   mining software tools, such experimental data needs
                   to be annotated with relevant metadata giving
                   information as to provenance, content, conditions and
                   so on. The need to automate the process of going from
                   raw data to information to knowledge is briefly
                   discussed. The paper argues the case for creating new
                   types of digital libraries for scientific data with
                   the same sort of management services as conventional
                   digital libraries in addition to other data-specific
                   services. Some likely implications of both the Open
                   Archives Initiative and e-Science data for the future
                   role for university libraries are briefly mentioned.
                   A substantial subset of this e-Science data needs to
                   archived and curated for long-term preservation. Some
                   of the issues involved in the digital preservation of
                   both scientific data and of the programs needed to
                   interpret the data are reviewed. Finally, the
                   implications of this wealth of e-Science data for the
                   Grid middleware infrastructure are highlighted.},
  doi =           {10.1002/0470867167.ch36},
  isbn =          {978-0-470-85319-1 978-0-470-86716-7},
  language =      {en},
}

@book{DSwadeBabbage2001,
  author =        {Swade, Doron and Babbage, Charles},
  publisher =     {{Viking Penguin}},
  title =         {Difference {{Engine}}: {{Charles Babbage}} and the
                   {{Quest}} to {{Build}} the {{First Computer}}},
  year =          {2001},
  abstract =      {From the Publisher: In 1821 an inventor and
                   mathematician, Charles Babbage, was poring over a set
                   of mathematical tables. Finding error after error
                   Babbage exclaimed, "I wish to God these calculations
                   had been executed by steam." His frustration was not
                   simply at the grindingly tedious labor of checking
                   manually evaluated tables, but at their daunting
                   unreliability. Science, engineering, construction,
                   banking, and insurance depended on tables for
                   calculation. Ships navigating by the stars relied on
                   them to find their positions at sea. Babbage launched
                   himself on a grand venture to design and build
                   mechanical calculating engines that would eliminate
                   such errors. His bid to build infallible machines is
                   a saga of ingenuity and will, which led beyond
                   mechanized arithmetic into the entirely new realm of
                   computing. Through Ada, Countess of Lovelace and
                   daughter of Lord Byron, we gain tantalizing insights
                   into how at least one Victorian glimpsed the promise
                   of what was to come. Babbage springs out of history
                   like a jack-in-the-box: a gentleman philosopher, a
                   tireless inventor, a vigorous socialite, and a
                   mesmerizing raconteur. "Mr. Babbage is coming to
                   dinner" was a coup for any hostess. Drawing on
                   previously unused archival material, The Difference
                   Engine is a tale of both Babbage's nineteenth-century
                   quest to build a calculating engine and its
                   twentieth-century sequel. For in 1991, Babbage's
                   vision was finally realized, at least in part, by the
                   completion at the Science Museum in London of the
                   first full-sized Babbage engine, finished in time for
                   the 200th anniversary of Babbage's birth. The two
                   quests are mutually illuminating and are recounted
                   here by the-then Curator of Computing, Doron Swade --
                   one of the main protagonists of the successful
                   resumption of Babbage's extraordinary work.},
  isbn =          {978-0-641-53440-9},
}

@article{RTryon1958a,
  author =        {Tryon, Robert C.},
  journal =       {Educational and Psychological Measurement},
  month =         oct,
  number =        {3},
  pages =         {477--495},
  publisher =     {{SAGE Publications Inc}},
  title =         {General {{Dimensions}} of {{Individual Differences}}:
                   {{Cluster Analysis Vs}}. {{Multiple Factor
                   Analysis}}},
  volume =        {18},
  year =          {1958},
  doi =           {10.1177/001316445801800304},
  issn =          {0013-1644},
}

@article{ABurks1947,
  author =        {Burks, A.W.},
  journal =       {Proceedings of the IRE},
  month =         aug,
  number =        {8},
  pages =         {756--767},
  title =         {Electronic {{Computing Circuits}} of the {{ENIAC}}},
  volume =        {35},
  year =          {1947},
  abstract =      {The ENIAC (Electronic Numerical Integrator and
                   Computer), the first electronic computing machine to
                   be built, is a very large device (containing 18,000
                   vacuum tubes) compounded out of a few basic types of
                   computing circuits. The design principles that were
                   followed in order to insure reliable operation of the
                   electronic computer are presented, and the basic
                   types of computing circuits are analyzed. Most of the
                   design work on component circuits was devoted to
                   constructing reliable memory circuits (flip-flops)
                   and adding circuits (counters). These are treated in
                   detail. The ENIAC performs the operations of
                   addition, subtraction, multiplication, division,
                   square-rooting, and the looking up of function values
                   automatically. The units which perform these
                   operations, the units which take numerical data into
                   and out of the machine, and those which control the
                   over-all operation are described. The technique of
                   combining the basic electronic circuits to perform
                   these functions is illustrated by three typical
                   computing circuits: the addition circuit, a
                   programming circuit, and the multiplication circuit.},
  doi =           {10.1109/JRPROC.1947.234265},
  issn =          {2162-6634},
}

@article{GMoore1998,
  author =        {Moore, G.E.},
  journal =       {Proceedings of the IEEE},
  month =         jan,
  number =        {1},
  pages =         {53--62},
  title =         {The Role of {{Fairchild}} in Silicon Technology in
                   the Early Days of "{{Silicon Valley}}"},
  volume =        {86},
  year =          {1998},
  abstract =      {Fairchild Semiconductor was founded in 1957 by a
                   group originating from Shockley Semiconductor
                   Laboratory, the first organization attempting to
                   exploit silicon transistor technology in the region
                   at the base of the San Francisco peninsula now often
                   referred to as "Silicon Valley". Fairchild produced
                   the first commercial silicon mesa transistors and
                   invented the "planar" process that formed the basis
                   of practical integrated circuits. Several of the key
                   directions in silicon device technology originated at
                   Fairchild Semiconductor Corporation and its successor
                   organization, the Semiconductor Division of Fairchild
                   Camera and Instrument Corporation. This paper
                   describes the author's recollections of some of the
                   related events.},
  doi =           {10.1109/5.658759},
  issn =          {1558-2256},
}

@article{DDonoho2017,
  author =        {Donoho, David},
  journal =       {Journal of Computational and Graphical Statistics},
  month =         oct,
  note =          {\_eprint:
                   https://doi.org/10.1080/10618600.2017.1384734},
  number =        {4},
  pages =         {745--766},
  publisher =     {{Taylor \& Francis}},
  title =         {50 {{Years}} of {{Data Science}}},
  volume =        {26},
  year =          {2017},
  abstract =      {More than 50 years ago, John Tukey called for a
                   reformation of academic statistics. In ``The Future
                   of Data Analysis,'' he pointed to the existence of an
                   as-yet unrecognized science, whose subject of
                   interest was learning from data, or ``data
                   analysis.'' Ten to 20 years ago, John Chambers, Jeff
                   Wu, Bill Cleveland, and Leo Breiman independently
                   once again urged academic statistics to expand its
                   boundaries beyond the classical domain of theoretical
                   statistics; Chambers called for more emphasis on data
                   preparation and presentation rather than statistical
                   modeling; and Breiman called for emphasis on
                   prediction rather than inference. Cleveland and Wu
                   even suggested the catchy name ``data science'' for
                   this envisioned field. A recent and growing
                   phenomenon has been the emergence of ``data science''
                   programs at major universities, including UC
                   Berkeley, NYU, MIT, and most prominently, the
                   University of Michigan, which in September 2015
                   announced a \$100M ``Data Science Initiative'' that
                   aims to hire 35 new faculty. Teaching in these new
                   programs has significant overlap in curricular
                   subject matter with traditional statistics courses;
                   yet many academic statisticians perceive the new
                   programs as ``cultural appropriation.'' This article
                   reviews some ingredients of the current ``data
                   science moment,'' including recent commentary about
                   data science in the popular media, and about
                   how/whether data science is really different from
                   statistics. The now-contemplated field of data
                   science amounts to a superset of the fields of
                   statistics and machine learning, which adds some
                   technology for ``scaling up'' to ``big data.'' This
                   chosen superset is motivated by commercial rather
                   than intellectual developments. Choosing in this way
                   is likely to miss out on the really important
                   intellectual event of the next 50 years. Because all
                   of science itself will soon become data that can be
                   mined, the imminent revolution in data science is not
                   about mere ``scaling up,'' but instead the emergence
                   of scientific studies of data analysis science-wide.
                   In the future, we will be able to predict how a
                   proposal to change data analysis workflows would
                   impact the validity of data analysis across all of
                   science, even predicting the impacts field-by-field.
                   Drawing on work by Tukey, Cleveland, Chambers, and
                   Breiman, I present a vision of data science based on
                   the activities of people who are ``learning from
                   data,'' and I describe an academic field dedicated to
                   improving that activity in an evidence-based manner.
                   This new field is a better academic enlargement of
                   statistics and machine learning than today's data
                   science initiatives, while being able to accommodate
                   the same short-term goals. Based on a presentation at
                   the Tukey Centennial Workshop, Princeton, NJ,
                   September 18, 2015.},
  doi =           {10.1080/10618600.2017.1384734},
  issn =          {1061-8600},
}

@article{GBell2008,
  author =        {Bell, Gordon},
  journal =       {IEEE Solid-State Circuits Society Newsletter},
  number =        {4},
  pages =         {8--19},
  title =         {Bell's {{Law}} for the {{Birth}} and {{Death}} of
                   {{Computer Classes}}: {{A}} Theory of the
                   {{Computer}}'s {{Evolution}}},
  volume =        {13},
  abstract =      {In 1951 a man could walk inside a computer. By 2010,
                   a computer cluster with millions of processors will
                   have expanded to building size. In this new paper
                   Gordon Bell explains the history of the computing
                   industry, positing a general theory ("Bell's Law) for
                   the creation, evolution, and death of computer
                   classes since 1951. Using the exponential transistor
                   density increases forecast by Moore's Law in 1965 and
                   1975 as the principal basis for the life cycle of
                   computer classes after the microprocessor was
                   introduced in 1971, he predicts that the powerful
                   microprocessor will be the basis for nearly all
                   computer classes in 2010, from personal computers and
                   servers costing a few thousand dollars to scalable
                   servers costing a few hundred million dollars. Soon
                   afterward, billions of cell phones for personal
                   computing, and tens of billions of wireless sensor
                   nets will unwire and interconnect everything.},
  doi =           {10.1109/N-SSC.2008.4785818},
  issn =          {1098-4232},
}

@book{JReynolds1998,
  author =        {Reynolds, John C.},
  publisher =     {{Cambridge University Press}},
  title =         {Theories of {{Programming Languages}}},
  year =          {1998},
  abstract =      {This textbook is a broad but rigorous survey of the
                   theoretical basis for the design, definition, and
                   implementation of programming languages, and of
                   systems for specifying and proving program behavior.
                   It encompasses imperative and functional programming,
                   as well as the ways of integrating these aspects into
                   more general languages. Basic concepts and their
                   properties are described with mathematical rigor, but
                   the mathematical development is balanced by numerous
                   examples of applications, particularly of program
                   specification and proof, concurrent programming,
                   functional programming (including the use of
                   continuations and lazy evaluation), and type systems
                   (including subtyping, polymorphism, and
                   modularization). Assuming only knowledge of
                   elementary programming, this text is perfect for
                   advanced undergraduate and beginning graduate courses
                   in programming language theory, and will also appeal
                   to researchers and professionals in designing or
                   implementing computer languages.},
  isbn =          {978-0-521-10697-9},
  language =      {en},
}

@article{JKermode2020,
  author =        {Kermode, James R.},
  journal =       {Journal of Physics: Condensed Matter},
  month =         may,
  number =        {30},
  pages =         {305901},
  publisher =     {{IOP Publishing}},
  title =         {F90wrap: An Automated Tool for Constructing Deep
                   {{Python}} Interfaces to Modern {{Fortran}} Codes},
  volume =        {32},
  year =          {2020},
  abstract =      {f90wrap is a tool to automatically generate Python
                   extension modules which interface to Fortran
                   libraries that makes use of derived types. It builds
                   on the capabilities of the popular f2py utility by
                   generating a simpler Fortran 90 interface to the
                   original Fortran code which is then suitable for
                   wrapping with f2py, together with a higher-level
                   Pythonic wrapper that makes the existance of an
                   additional layer transparent to the final user.
                   f90wrap has been used to wrap a number of large
                   software packages of relevance to the condensed
                   matter physics community, including the QUIP
                   molecular dynamics code and the CASTEP density
                   functional theory code.},
  doi =           {10.1088/1361-648X/ab82d2},
  issn =          {0953-8984},
  language =      {en},
}

@article{CShannon1948,
  author =        {Shannon, C. E.},
  journal =       {The Bell System Technical Journal},
  month =         jul,
  number =        {3},
  pages =         {379--423},
  title =         {A Mathematical Theory of Communication},
  volume =        {27},
  year =          {1948},
  abstract =      {The recent development of various methods of
                   modulation such as PCM and PPM which exchange
                   bandwidth for signal-to-noise ratio has intensified
                   the interest in a general theory of communication. A
                   basis for such a theory is contained in the important
                   papers of Nyquist1 and Hartley2 on this subject. In
                   the present paper we will extend the theory to
                   include a number of new factors, in particular the
                   effect of noise in the channel, and the savings
                   possible due to the statistical structure of the
                   original message and due to the nature of the final
                   destination of the information.},
  doi =           {10.1002/j.1538-7305.1948.tb01338.x},
  issn =          {0005-8580},
}

@article{ASingletonArribas-Bel2019,
  author =        {Singleton, Alex and Arribas-Bel, Daniel},
  journal =       {Geographical Analysis},
  note =          {\_eprint:
  https://onlinelibrary.wiley.com/doi/pdf/10.1111/gean.12194},
  number =        {0},
  pages =         {1--15},
  title =         {Geographic {{Data Science}}},
  volume =        {2019},
  year =          {2019},
  abstract =      {It is widely acknowledged that the emergence of ``Big
                   Data'' is having a profound and often controversial
                   impact on the production of knowledge. In this
                   context, Data Science has developed as an
                   interdisciplinary approach that turns such ``Big
                   Data'' into information. This article argues for the
                   positive role that Geography can have on Data Science
                   when being applied to spatially explicit problems;
                   and inversely, makes the case that there is much that
                   Geography and Geographical Analysis could learn from
                   Data Science. We propose a deeper integration through
                   an ambitious research agenda, including systems
                   engineering, new methodological development, and work
                   toward addressing some acute challenges around
                   epistemology. We argue that such issues must be
                   resolved in order to realize a Geographic Data
                   Science, and that such goal would be a desirable
                   one.},
  doi =           {10.1111/gean.12194},
  issn =          {1538-4632},
  language =      {en},
}

@incollection{RThiele2005,
  address =       {{New York, NY}},
  author =        {Thiele, R{\"u}diger},
  booktitle =     {Mathematics and the {{Historian}}'s {{Craft}}},
  editor =        {Borwein, Jonathan and Dilcher, Karl and
                   Van Brummelen, Glen and Kinyon, Michael},
  pages =         {243--295},
  publisher =     {{Springer New York}},
  title =         {Hilbert and His {{Twenty}}-{{Four Problems}}},
  year =          {2005},
  doi =           {10.1007/0-387-28272-6_11},
  isbn =          {978-0-387-25284-1 978-0-387-28272-5},
  language =      {en},
}

@article{JTukey1962,
  author =        {Tukey, John W.},
  journal =       {The Annals of Mathematical Statistics},
  month =         mar,
  number =        {1},
  pages =         {1--67},
  publisher =     {{Institute of Mathematical Statistics}},
  title =         {The {{Future}} of {{Data Analysis}}},
  volume =        {33},
  year =          {1962},
  abstract =      {Project Euclid - mathematics and statistics online},
  doi =           {10.1214/aoms/1177704711},
  issn =          {0003-4851, 2168-8990},
  language =      {EN},
}

@book{DKnuth2014,
  author =        {Knuth, Donald E.},
  month =         may,
  publisher =     {{Addison-Wesley Professional}},
  title =         {Art of {{Computer Programming}}, {{Volume}} 2:
                   {{Seminumerical Algorithms}}},
  year =          {2014},
  abstract =      {The bible of all fundamental algorithms and the work
                   that taught many of today's software developers most
                   of what they know about computer programming.
                   \textemdash{}Byte, September 1995 I can't begin to
                   tell you how many pleasurable hours of study and
                   recreation they have afforded me! I have pored over
                   them in cars, restaurants, at work, at home... and
                   even at a Little League game when my son wasn't in
                   the line-up. \textemdash{}Charles Long If you think
                   you're a really good programmer... read [Knuth's] Art
                   of Computer Programming... You should definitely send
                   me a resume if you can read the whole thing.
                   \textemdash{}Bill Gates It's always a pleasure when a
                   problem is hard enough that you have to get the
                   Knuths off the shelf. I find that merely opening one
                   has a very useful terrorizing effect on computers.
                   \textemdash{}Jonathan Laventhol The second volume
                   offers a complete introduction to the field of
                   seminumerical algorithms, with separate chapters on
                   random numbers and arithmetic. The book summarizes
                   the major paradigms and basic theory of such
                   algorithms, thereby providing a comprehensive
                   interface between computer programming and numerical
                   analysis. Particularly noteworthy in this third
                   edition is Knuth's new treatment of random number
                   generators, and his discussion of calculations with
                   formal power series.},
  isbn =          {978-0-321-63576-1},
  language =      {en},
}

@article{LMenabreaLovelace1843,
  author =        {Menabrea, L. and Lovelace, A.},
  journal =       {Scientific Memoirs},
  number =        {3},
  title =         {Sketch of the {{Analytical Engine Invented}} by
                   {{Charles Babbage}} ({{English Translation}})},
  year =          {1843},
}

@article{SJohnsonRitchie1978,
  author =        {Johnson, S. C. and Ritchie, D. M.},
  journal =       {The Bell System Technical Journal},
  month =         jul,
  number =        {6},
  pages =         {2021--2048},
  title =         {{{UNIX}} Time-Sharing System: {{Portability}} of {{C
                   Programs}} and the {{UNIX}} System},
  volume =        {57},
  year =          {1978},
  abstract =      {Computer programs are portable to the extent that
                   they can be moved to new computing environments with
                   much less effort than it would take to rewrite them.
                   In the limit, a program is perfectly portable if it
                   can be moved at will with no change whatsoever.
                   Recent C language extensions have made it easier to
                   write portable programs. Some tools have also been
                   developed that aid in the detection of nonportable
                   constructions. With these tools many programs have
                   been moved from the PDP-11 on which they were
                   developed to other machines. In particular, the
                   UNIX{${_\ast}$} operating system and most of its
                   software have been transported to the Interdata 8/32.
                   The source-language representation of most of the
                   code involved is identical in all environments.},
  doi =           {10.1002/j.1538-7305.1978.tb02141.x},
  issn =          {0005-8580},
}

@misc{ERaymond2000,
  author =        {Raymond, Eric Steven},
  publisher =     {{Thyrsus Enterprises}},
  title =         {A {{Brief History}} of {{Hackerdom}}},
  year =          {2000},
  language =      {en},
}

@inproceedings{JBackusEtAl1957,
  address =       {{New York, NY, USA}},
  author =        {Backus, J. W. and Beeber, R. J. and Best, S. and
                   Goldberg, R. and Haibt, L. M. and Herrick, H. L. and
                   Nelson, R. A. and Sayre, D. and Sheridan, P. B. and
                   Stern, H. and Ziller, I. and Hughes, R. A. and
                   Nutt, R.},
  booktitle =     {Papers Presented at the {{February}} 26-28, 1957,
                   Western Joint Computer Conference: {{Techniques}} for
                   Reliability},
  month =         feb,
  pages =         {188--198},
  publisher =     {{Association for Computing Machinery}},
  series =        {{{IRE}}-{{AIEE}}-{{ACM}} '57 ({{Western}})},
  title =         {The {{FORTRAN}} Automatic Coding System},
  year =          {1957},
  abstract =      {The FORTRAN project was begun in the summer of 1954.
                   Its purpose was to reduce by a large factor the task
                   of preparing scientific problems for IBM's next large
                   computer, the 704. If it were possible for the 704 to
                   code problems for itself and produce as good programs
                   as human coders (but without the errors), it was
                   clear that large benefits could be achieved. For it
                   was known that about two-thirds of the cost of
                   solving most scientific and engineering problems on
                   large computers was that of problem preparation.
                   Furthermore, more than 90 per cent of the elapsed
                   time for a problem was usually devoted to planning,
                   writing, and debugging the program. In many cases the
                   development of a general plan for solving a problem
                   was a small job in comparison to the task of devising
                   and coding machine procedures to carry out the plan.
                   The goal of the FORTRAN project was to enable the
                   programmer to specify a numerical procedure using a
                   concise language like that of mathematics and obtain
                   automatically from this specification an efficient
                   704 program to carry out the procedure. It was
                   expected that such a system would reduce the coding
                   and debugging task to less than one-fifth of the job
                   it had been.},
  doi =           {10.1145/1455567.1455599},
  isbn =          {978-1-4503-7861-1},
}

@article{RHarrisEtAl2017,
  author =        {Harris, Richard and O'Sullivan, David and
                   Gahegan, Mark and Charlton, Martin and Comber, Lex and
                   Longley, Paul and Brunsdon, Chris and Malleson, Nick and
                   Heppenstall, Alison and Singleton, Alex and
                   {Arribas-Bel}, Daniel and Evans, Andy},
  journal =       {Environment and Planning B: Urban Analytics and City
                   Science},
  month =         jul,
  number =        {4},
  pages =         {598--617},
  publisher =     {{SAGE Publications Ltd STM}},
  title =         {More Bark than Bytes? {{Reflections}} on 21+ Years of
                   Geocomputation},
  volume =        {44},
  year =          {2017},
  abstract =      {This year marks the 21st anniversary of the
                   International GeoComputation Conference Series. To
                   celebrate the occasion, Environment and Planning B
                   invited some members of the geocomputational
                   community to reflect on its achievements, some of the
                   unrealised potential, and to identify some of the
                   on-going challenges.},
  doi =           {10.1177/2399808317710132},
  issn =          {2399-8083},
  language =      {en},
}

@article{ABromley1982,
  author =        {Bromley, A. G.},
  journal =       {Annals of the History of Computing},
  number =        {3},
  pages =         {196--217},
  title =         {Charles {{Babbage}}'s {{Analytical Engine}}, 1838},
  volume =        {4},
  year =          {1982},
  language =      {en},
}

@book{MMinskyPapert1969,
  author =        {Minsky, Marvin Lee and Papert, Seymour},
  publisher =     {{MIT Press}},
  title =         {Perceptrons: {{An Introduction}} to {{Computational
                   Geometry}}},
  year =          {1969},
  abstract =      {Perceptrons - the first systematic study of
                   parallelism in computation - has remaineda classical
                   work on threshold automata networks for nearly two
                   decades. It marked a historical turnin artificial
                   intelligence, and it is required reading for anyone
                   who wants to understand theconnectionist
                   counterrevolution that is going on
                   today.Artificial-intelligence research, which for
                   atime concentrated on the programming of ton Neumann
                   computers, is swinging back to the idea
                   thatintelligence might emerge from the activity of
                   networks of neuronlike entities. Minsky and
                   Papert'sbook was the first example of a mathematical
                   analysis carried far enough to show the
                   exactlimitations of a class of computing machines
                   that could seriously be considered as models of
                   thebrain. Now the new developments in mathematical
                   tools, the recent interest of physicists in thetheory
                   of disordered matter, the new insights into and
                   psychological models of how the brain works,and the
                   evolution of fast computers that can simulate
                   networks of automata have given Perceptronsnew
                   importance.Witnessing the swing of the intellectual
                   pendulum, Minsky and Papert have added a newchapter
                   in which they discuss the current state of parallel
                   computers, review developments since theappearance of
                   the 1972 edition, and identify new research
                   directions related to connectionism. Theynote a
                   central theoretical challenge facing connectionism:
                   the challenge to reach a deeperunderstanding of how
                   "objects" or "agents" with individuality can emerge
                   in a network. Progress inthis area would link
                   connectionism with what the authors have called
                   "society theories ofmind."Marvin L. Minsky is Donner
                   Professor of Science in MIT's Electrical Engineering
                   and ComputerScience Department. Seymour A. Papert is
                   Professor of Media Technology at MIT.},
  language =      {en},
}

@article{DSollaPrice1984,
  author =        {Solla Price, Derek},
  journal =       {IEEE Micro},
  month =         feb,
  number =        {1},
  pages =         {22--52},
  title =         {A {{History}} of {{Calculating Machines}}},
  volume =        {4},
  year =          {1984},
  doi =           {10.1109/MM.1984.291305},
  issn =          {0272-1732},
}

@inbook{DSwade2012,
  author =        {Swade, Doron},
  booktitle =     {A {{Computable Universe}}},
  month =         dec,
  pages =         {23--43},
  publisher =     {{WORLD SCIENTIFIC}},
  title =         {Origins of {{Digital Computing}}: {{Alan Turing}},
                   {{Charles Babbage}}, and {{Ada Lovelace}}},
  year =          {2012},
  doi =           {10.1142/9789814374309_0002},
  isbn =          {978-981-4374-29-3 978-981-4374-30-9},
  language =      {en},
}

@book{SOpenshawOpenshaw1977,
  author =        {Openshaw, Stan and Openshaw, Christine},
  publisher =     {{Wiley}},
  title =         {Artificial {{Intelligence}} in {{Geography}}},
  year =          {1977},
  abstract =      {This unique work introduces the basic principles of
                   artificialintelligence with applications in
                   geographical teaching andresearch, GIS, and planning.
                   Written in an accessible,non-technical and witty
                   style, this book marks the beginning of theAl
                   revolution in geography with major implications for
                   teaching andresearch. The authors provide an easy to
                   understand basicintroduction to Al relevant to
                   geography. There are no specialmathematical and
                   statistical skills needed, indeed these might wellbe
                   a hindrance. Al is a different way of looking at the
                   world andit requires a willingness to experiment, and
                   readers who areunhindered by the baggage of obsolete
                   technologies and outmodedphilosophies of science will
                   probably do best. The text provides anintroduction to
                   expert systems, neural nets, genetic algorithms,smart
                   systems and artificial life and shows how they are
                   likely totransform geographical enquiry. * A major
                   methodological milestone in geography * The first
                   geographical book on artificial intelligence (Al) *
                   No need for previous mathematical or
                   statisticalskills/knowledge * Accessible style makes
                   a difficult subject available to a wideaudience *
                   Stan Openshaw is one of the world s leading
                   researchers intogeographical computing, spatial
                   analysis and GIS.},
  isbn =          {978-0-471-96991-4},
  language =      {en},
}

@book{AThackrayEtAl2015,
  author =        {Thackray, Arnold and Brock, David C. and
                   Jones, Rachel},
  month =         may,
  publisher =     {{Hachette UK}},
  title =         {Moore's {{Law}}: {{The Life}} of {{Gordon Moore}},
                   {{Silicon Valley}}'s {{Quiet Revolutionary}}},
  year =          {2015},
  abstract =      {Our world today-from the phone in your pocket to the
                   car that you drive, the allure of social media to the
                   strategy of the Pentagon-has been shaped irrevocably
                   by the technology of silicon transistors. Year after
                   year, for half a century, these tiny switches have
                   enabled ever-more startling capabilities. Their
                   incredible proliferation has altered the course of
                   human history as dramatically as any political or
                   social revolution. At the heart of it all has been
                   one quiet Californian: Gordon Moore.At Fairchild
                   Semiconductor, his seminal Silicon Valley startup,
                   Moore-a young chemist turned electronics
                   entrepreneur-had the defining insight: silicon
                   transistors, and microchips made of them, could make
                   electronics profoundly cheap and immensely powerful.
                   Microchips could double in power, then redouble again
                   in clockwork fashion. History has borne out this
                   insight, which we now call "Moore's Law", and Moore
                   himself, having recognized it, worked endlessly to
                   realize his vision. With Moore's technological
                   leadership at Fairchild and then at his second
                   start-up, the Intel Corporation, the law has held for
                   fifty years. The result is profound: from the days of
                   enormous, clunky computers of limited capability to
                   our new era, in which computers are placed everywhere
                   from inside of our bodies to the surface of
                   Mars.Moore led nothing short of a revolution. In
                   Moore's Law, Arnold Thackray, David C. Brock, and
                   Rachel Jones give the authoritative account of Gordon
                   Moore's life and his role in the development both of
                   Silicon Valley and the transformative technologies
                   developed there. Told by a team of writers with
                   unparalleled access to Moore, his family, and his
                   contemporaries, this is the human story of man and a
                   career that have had almost superhuman effects. The
                   history of twentieth-century technology is littered
                   with overblown "revolutions." Moore's Law is
                   essential reading for anyone seeking to learn what a
                   real revolution looks like.},
  isbn =          {978-0-465-05562-3},
  language =      {en},
}

@misc{SOpenshawAbrahart1996,
  author =        {Openshaw, S. and Abrahart, R.},
  title =         {Geocomputation},
  year =          {1996},
}

@book{RTryon1955,
  author =        {Tryon, R. C.},
  publisher =     {{University of California Press}},
  title =         {Identification of {{Social Areas}} by {{Cluster
                   Analysis}}: A General Method with an Application to
                   the {{San Francisco Bay Area}}},
  year =          {1955},
}

@article{DKnuth1984,
  author =        {Knuth, Donald Ervin},
  journal =       {The Computer Journal},
  number =        {2},
  pages =         {97---111.},
  title =         {Literate Programming},
  volume =        {27},
  year =          {1984},
  abstract =      {The author and his associates have been experimenting
                   for the past several years with a programming
                   language and documentation system called WEB. This
                   paper presents WEB by example, and discusses why the
                   new system appears to be an improvement over previous
                   ones.},
}

@book{CBishop2006,
  address =       {{New York}},
  author =        {Bishop, Christopher M.},
  publisher =     {{Springer}},
  series =        {Information Science and Statistics},
  title =         {Pattern Recognition and Machine Learning},
  year =          {2006},
  isbn =          {978-0-387-31073-2},
  language =      {en},
}

@article{WMcCullochPitts1943,
  author =        {McCulloch, Warren S and Pitts, Walter},
  journal =       {Bulletin of Mathematical Biophysics},
  pages =         {17},
  title =         {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS
                   ACTIVITY}}},
  year =          {1943},
  language =      {en},
}

@article{CMack2011,
  author =        {Mack, Chris A.},
  journal =       {IEEE Transactions on Semiconductor Manufacturing},
  month =         may,
  number =        {2},
  pages =         {202--207},
  title =         {Fifty {{Years}} of {{Moore}}'s {{Law}}},
  volume =        {24},
  year =          {2011},
  abstract =      {The 1959 invention of the planar silicon transistor
                   led to the development of the integrated circuit (IC)
                   and the growth trend in IC complexity known as
                   Moore's Law. While Moore's observation came in 1965,
                   his original trend line showing a doubling of
                   components per chip each year began with one
                   component in 1959. Thus, we have now experienced 50
                   years of Moore's Law. This paper provides a history
                   of Moore's Law through its many changes and
                   reinterpretations, containing possibly a few new ones
                   as well.},
  doi =           {10.1109/TSM.2010.2096437},
  issn =          {1558-2345},
}

@incollection{VMullerBostrom2016,
  address =       {{Cham}},
  author =        {M{\"u}ller, Vincent C. and Bostrom, Nick},
  booktitle =     {Fundamental {{Issues}} of {{Artificial
                   Intelligence}}},
  editor =        {M{\"u}ller, Vincent C.},
  pages =         {555--572},
  publisher =     {{Springer International Publishing}},
  series =        {Synthese {{Library}}},
  title =         {Future {{Progress}} in {{Artificial Intelligence}}:
                   {{A Survey}} of {{Expert Opinion}}},
  year =          {2016},
  abstract =      {There is, in some quarters, concern about
                   high\textendash{}level machine intelligence and
                   superintelligent AI coming up in a few decades,
                   bringing with it significant risks for humanity. In
                   other quarters, these issues are ignored or
                   considered science fiction. We wanted to clarify what
                   the distribution of opinions actually is, what
                   probability the best experts currently assign to
                   high\textendash{}level machine intelligence coming up
                   within a particular time\textendash{}frame, which
                   risks they see with that development, and how fast
                   they see these developing. We thus designed a brief
                   questionnaire and distributed it to four groups of
                   experts in 2012/2013. The median estimate of
                   respondents was for a one in two chance that
                   high-level machine intelligence will be developed
                   around 2040\textendash{}2050, rising to a nine in ten
                   chance by 2075. Experts expect that systems will move
                   on to superintelligence in less than 30 years
                   thereafter. They estimate the chance is about one in
                   three that this development turns out to be `bad' or
                   `extremely bad' for humanity.},
  doi =           {10.1007/978-3-319-26485-1_33},
  isbn =          {978-3-319-26485-1},
  language =      {en},
}

@article{ATuring1950,
  author =        {Turing, Alan},
  journal =       {Mind},
  title =         {Computing {{Machinery}} and {{Intelligence}}},
  year =          {1950},
}

@book{GVanRossumDrakeJr1995,
  author =        {Van Rossum, Guido and Drake Jr, Fred L.},
  publisher =     {{Centrum voor Wiskunde en Informatica Amsterdam}},
  title =         {Python Reference Manual},
  year =          {1995},
}

@article{RBlashfield1980,
  author =        {Blashfield, Roger K.},
  journal =       {Multivariate Behavioral Research},
  month =         oct,
  number =        {4},
  pages =         {439--458},
  title =         {The {{Growth Of Cluster Analysis}}: {{Tryon}},
                   {{Ward}}, {{And Johnson}}},
  volume =        {15},
  year =          {1980},
  doi =           {10.1207/s15327906mbr1504_4},
  issn =          {0027-3171, 1532-7906},
  language =      {en},
}

@book{ERaymond2001,
  author =        {Raymond, Eric S.},
  edition =       {1 edition},
  month =         feb,
  publisher =     {{O'Reilly Media}},
  title =         {The {{Cathedral}} \& the {{Bazaar}}: {{Musings}} on
                   {{Linux}} and {{Open Source}} by an {{Accidental
                   Revolutionary}}},
  year =          {2001},
  abstract =      {Open source provides the competitive advantage in the
                   Internet Age. According to the August Forrester
                   Report, 56 percent of IT managers interviewed at
                   Global 2,500 companies are already using some type of
                   open source software in their infrastructure and
                   another 6 percent will install it in the next two
                   years. This revolutionary model for collaborative
                   software development is being embraced and studied by
                   many of the biggest players in the high-tech
                   industry, from Sun Microsystems to IBM to Intel.The
                   Cathedral \& the Bazaar is a must for anyone who
                   cares about the future of the computer industry or
                   the dynamics of the information economy. Already,
                   billions of dollars have been made and lost based on
                   the ideas in this book. Its conclusions will be
                   studied, debated, and implemented for years to come.
                   According to Bob Young, "This is Eric Raymond's great
                   contribution to the success of the open source
                   revolution, to the adoption of Linux-based operating
                   systems, and to the success of open source users and
                   the companies that supply them."The interest in open
                   source software development has grown enormously in
                   the past year. This revised and expanded paperback
                   edition includes new material on open source
                   developments in 1999 and 2000. Raymond's clear and
                   effective writing style accurately describing the
                   benefits of open source software has been key to its
                   success. With major vendors creating acceptance for
                   open source within companies, independent vendors
                   will become the open source story in 2001.},
  language =      {English},
}

@article{DBrock2012,
  author =        {Brock, David C.},
  journal =       {History and Technology},
  month =         dec,
  note =          {\_eprint:
                   https://doi.org/10.1080/07341512.2012.756236},
  number =        {4},
  pages =         {375--401},
  publisher =     {{Routledge}},
  title =         {From Automation to {{Silicon Valley}}: The Automation
                   Movement of the 1950s, {{Arnold Beckman}}, and
                   {{William Shockley}}},
  volume =        {28},
  year =          {2012},
  abstract =      {Most studies mark the start of silicon electronics in
                   Silicon Valley with William Shockley and Arnold
                   Beckman's creation of the Shockley Semiconductor
                   Laboratory. This study details how the automation
                   movement of the 1950s shaped the careers of both
                   Shockley and Beckman, and formed an indispensible
                   context for their creation of Shockley Semiconductor.
                   Shockley was engaged in automation from the early
                   1950s, promoting his vision of an `automatic
                   trainable robot' to revolutionize manufacturing.
                   Beckman was deeply involved in automation in the
                   mid-1950s, orienting his company to key technologies
                   for the `automatic factory:' instrumentation and
                   computers. Beckman and Shockley's entrepreneurial
                   involvements with electronics and automation led them
                   to create Shockley Semiconductor to pursue silicon
                   transistors in 1955.},
  doi =           {10.1080/07341512.2012.756236},
  issn =          {0734-1512},
}

@misc{HGoldstinevonNeumann1948,
  author =        {Goldstine, H. H. and {von Neumann}, J.},
  title =         {Planning and {{Coding}} of {{Problems}} for an
                   {{Electronic Computing Instrument}}},
  year =          {1948},
}

@misc{LTorvalds1991,
  author =        {Torvalds, Linus},
  title =         {Linux\textemdash{}a Free Unix-386 Kernel},
  year =          {1991},
  language =      {en},
}

@article{DArribas-BelReades2018,
  author =        {Arribas-Bel, Dani and Reades, Jon},
  journal =       {Geography Compass},
  note =          {\_eprint:
  https://onlinelibrary.wiley.com/doi/pdf/10.1111/gec3.12403},
  number =        {10},
  pages =         {e12403},
  title =         {Geography and Computers: {{Past}}, Present, and
                   Future},
  volume =        {12},
  year =          {2018},
  abstract =      {The discipline of Geography has long been intertwined
                   with the use of computers. This close interaction is
                   likely to increase with the embeddedness of computers
                   and concomitant growth of spatially referenced data.
                   To better understand the current situation, and to be
                   able to better speculate about the future, this
                   article provides two parallel perspectives: first, we
                   offer an historical perspective on the relationship
                   between Geography and computers; second, we document
                   developments\textemdash{}in particular the nascent
                   field of data science\textemdash{}that are currently
                   taking place outside of Geography and to which we
                   argue the discipline should be paying close
                   attention. Combining both perspectives, we identify
                   the benefits of tighter integration between Geography
                   and Data Science and argue for the establishment of a
                   new space\textemdash{}that we term Geographic Data
                   Science\textemdash{}in which cross-pollination could
                   occur to the benefit of both Geography and the larger
                   data community.},
  doi =           {10.1111/gec3.12403},
  issn =          {1749-8198},
  language =      {en},
}

@book{JEssinger2004,
  author =        {Essinger, James},
  month =         oct,
  publisher =     {{OUP Oxford}},
  title =         {Jacquard's {{Web}}: {{How}} a Hand-Loom Led to the
                   Birth of the Information Age},
  year =          {2004},
  abstract =      {Jacquard's Web is the story of some of the most
                   ingenious inventors the world has ever known, a
                   fascinating account of how a hand-loom invented in
                   Napoleonic France led to the development of the
                   modern information age. James Essinger, a master
                   story-teller, shows through a series of remarkable
                   and meticulously researched historical connections
                   (spanning two centuries and never investigated
                   before) that the Jacquard loom kick-started a process
                   of scientific evolution which would lead directly to
                   the development of the modern computer. The invention
                   of Jacquard's loom in 1804 enabled the master
                   silk-weavers of Lyons to weave fabrics 25 times
                   faster than had previously been possible. The device
                   used punched cards, which stored instructions for
                   weaving whatever pattern or design was required; it
                   proved an outstanding success. These cards can very
                   reasonably be described as the world's first computer
                   programmes. In this engaging and delightful book,
                   James Essinger reveals a plethora of extraordinary
                   links between the nineteenth-century world of weaving
                   and today's computer age: to give just one example,
                   modern computer graphics displays are based on
                   exactly the same principles as those employed in
                   Jacquard's special woven tableaux. Jacquard's Web
                   also introduces some of the most colourful and
                   interesting characters in the history of science and
                   technology: the modest but exceptionally dedicated
                   Jacquard himself, the brilliant but temperamental
                   Victorian polymath Charles Babbage, who dreamt of a
                   cogwheel computer operated using Jacquard cards, and
                   the imaginative and perceptive Ada Lovelace, Lord
                   Byron's only legitimate daughter.},
  isbn =          {978-0-19-151725-9},
  language =      {en},
}

@article{WTobler1970,
  author =        {Tobler, W. R.},
  journal =       {Economic Geography},
  month =         jun,
  note =          {\_eprint:
                   https://www.tandfonline.com/doi/pdf/10.2307/143141},
  number =        {sup1},
  pages =         {234--240},
  publisher =     {{Routledge}},
  title =         {A {{Computer Movie Simulating Urban Growth}} in the
                   {{Detroit Region}}},
  volume =        {46},
  year =          {1970},
  doi =           {10.2307/143141},
  issn =          {0013-0095},
}

@article{MGoodchild1992,
  author =        {Goodchild, Michael F.},
  journal =       {International Journal of Geographical Information
                   Systems},
  month =         jan,
  note =          {\_eprint: https://doi.org/10.1080/02693799208901893},
  number =        {1},
  pages =         {31--45},
  publisher =     {{Taylor \& Francis}},
  title =         {Geographical Information Science},
  volume =        {6},
  year =          {1992},
  abstract =      {. Research papers at conferences such as EGIS and the
                   International Symposia on Spatial Data Handling
                   address a set of intellectual and scientific
                   questions which go well beyond the limited technical
                   capabilities of current technology in geographical
                   information systems. This paper reviews the topics
                   which might be included in a science of geographical
                   information. Research on these fundamental issues is
                   a better prospect for long-term survival and
                   acceptance in the academy than the development of
                   technical capabilities. This paper reviews the
                   current state of research in a series of key areas
                   and speculates on why progress has been so uneven.
                   The final section of the paper looks to the future
                   and to new areas of significant potential in this
                   area of research.},
  doi =           {10.1080/02693799208901893},
  issn =          {0269-3798},
}

@article{PWegner1976,
  author =        {Wegner, P.},
  journal =       {IEEE Transactions on Computers},
  month =         dec,
  number =        {12},
  pages =         {1207--1225},
  title =         {Programming {{Languages}}\textemdash{{The First}} 25
                   {{Years}}},
  volume =        {C-25},
  year =          {1976},
  abstract =      {The programming language field is certainly one of
                   the most important subfields of computer science. It
                   is rich in concepts, theories, and practical
                   developments. The present paper attempts to trace the
                   25 year development of programming languages by means
                   of a sequence of 30 milestones (languages and
                   concepts) listed in more or less historical order.
                   The first 13 milestones (M1\textendash{}M13) are
                   largely concerned with specific programming languages
                   of the 1950's and 1960's such as Fortran, Algol 60,
                   Cobol, Lisp, and Snobol 4. The next ten milestones
                   (M14\textendash{}M23) relate to concepts and theories
                   in the programming language field such as formal
                   language theory, language definition, program
                   verification, semantics and abstraction. The
                   remaining milestones (M24\textendash{}M30) relate to
                   the software engineering methodology of the 1970's
                   and include a discussion of structured programming
                   and the life cycle concept. This discussion of
                   programming language development is far from complete
                   and there are both practical developments such as
                   special purpose languages and theoretical topics such
                   as the lambda calculus which are not adequately
                   covered. However, it is hoped that the discussion
                   covers the principal concepts and languages in a
                   reasonably nontrivial way and that it captures the
                   sense of excitement and the enormous variety of
                   activity that was characteristic of the programming
                   language field during its first 25 years.},
  doi =           {10.1109/TC.1976.1674589},
  issn =          {1557-9956},
}

@article{JTukey1958,
  author =        {Tukey, John W.},
  journal =       {The American Mathematical Monthly},
  month =         jan,
  number =        {1},
  pages =         {1--9},
  title =         {The {{Teaching}} of {{Concrete Mathematics}}},
  volume =        {65},
  year =          {1958},
  doi =           {10.1080/00029890.1958.11989128},
  issn =          {0002-9890, 1930-0972},
  language =      {en},
}

@incollection{ICohen1990,
  address =       {{New York, NY, USA}},
  author =        {Cohen, I. Bernard},
  booktitle =     {A History of Scientific Computing},
  editor =        {Nash, Stephen G.},
  month =         jun,
  pages =         {41--52},
  publisher =     {{ACM}},
  title =         {Howard {{H}}. {{Aiken}} and the Computer},
  year =          {1990},
  doi =           {10.1145/87252.88066},
  isbn =          {978-0-201-50814-7},
  language =      {en},
}

@article{DRitchieThompson1978,
  author =        {Ritchie, D. M. and Thompson, K.},
  journal =       {Bell System Technical Journal},
  note =          {\_eprint:
  https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1978.tb02136.x},
  number =        {6},
  pages =         {1905--1929},
  title =         {The {{UNIX Time}}-{{Sharing System}}},
  volume =        {57},
  year =          {1978},
  abstract =      {unix* is a general-purpose, multi-user, interactive
                   operating system for the larger Digital Equipment
                   Corporation pdp-11 and the Interdata 8/32 computers.
                   It offers a number of features seldom found even in
                   larger operating systems, including (i) A
                   hierarchical file system incorporating demountable
                   volumes, (ii) Compatible file, device, and
                   inter-process I/O, (iii) The ability to initiate
                   asynchronous processes, (iv) System command language
                   selectable on a per-user basis, (v) Over 100
                   subsystems including a dozen languages, (vi) High
                   degree of portability. This paper discusses the
                   nature and implementation of the file system and of
                   the user command interface.},
  doi =           {10.1002/j.1538-7305.1978.tb02136.x},
  issn =          {1538-7305},
  language =      {en},
}

@book{ASilberschatzEtAl2011,
  author =        {Silberschatz, A. and Galvin, P.B. and Galvin, G.},
  publisher =     {{Wiley}},
  title =         {Operating {{System Concepts Essentials}}},
  year =          {2011},
}

@article{JBackus1978,
  author =        {Backus, John},
  journal =       {Association for Computing Machinery},
  number =        {8},
  title =         {{{THE HISTORY OF FORTRAN I}}, {{II}}, {{AND III}}},
  volume =        {13},
  year =          {1978},
  language =      {en},
}

@article{KSugden1981,
  author =        {Sugden, Keith F.},
  journal =       {Accounting Historians Journal},
  month =         sep,
  number =        {2},
  pages =         {1--22},
  publisher =     {{Allen Press}},
  title =         {A {{HISTORY OF THE ABACUS}}},
  volume =        {8},
  year =          {1981},
  doi =           {10.2308/0148-4184.8.2.1},
  issn =          {0148-4184},
  language =      {en},
}

@article{FAnscombe2003,
  author =        {Anscombe, F. R.},
  journal =       {Statistical Science},
  number =        {3},
  pages =         {287--310},
  publisher =     {{Institute of Mathematical Statistics}},
  title =         {Quiet {{Contributor}}: {{The Civic Career}} and
                   {{Times}} of {{John W}}. {{Tukey}}},
  volume =        {18},
  year =          {2003},
  abstract =      {Across 60 years, John W. Tukey contributed to the
                   advancement of democracy, peace and industry via
                   development, application and teaching of knowledge.
                   In his nation's service, he contributed to the Nike
                   missile defense, U-2 spy plane, surveillance
                   satellites in space, hydrophones in the oceans,
                   seismic data interpretation and communications code
                   breaking. As computer and communication pioneer,
                   Tukey collaborated with von Neumann, Shannon and
                   Pierce; coined "bit" and "software"; applied
                   statistical time series methods to processing
                   signals; and recognized the usefulness of fast
                   Fourier transform algorithms to digital processing of
                   correlated data. Practical problems inspired Tukey to
                   invent new ways to analyze data. As teacher and
                   author, he made these available to others. Tukey
                   advised government and industry regarding
                   environmental quality, educational testing, the
                   census, pharmaceutical efficacy, manufacturing
                   quality and technologies for gathering intelligence.
                   This paper explores the civic career, influences and
                   philosophies of a practicing data analyst, inventor
                   and remarkable public servant.},
  issn =          {0883-4237},
}

@article{MGoodchild1995,
  author =        {Goodchild, Michael F.},
  journal =       {International Journal of Geographical Information
                   Science},
  number =        {1},
  pages =         {1--7},
  title =         {Future {{Directions}} for {{Geographic Information
                   Science}}},
  volume =        {1},
  year =          {1995},
}

